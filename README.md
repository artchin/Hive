Комплекс задач по аналитике пользовательских логов новостных сайтов:

Задача 1 (Data Modeling): Создание схемы данных из 4 внешних (EXTERNAL) таблиц: логи запросов, данные пользователей, IP-регионы, подсети. Десериализация сырых данных через RegexSerDe. Партиционирование таблицы логов по датам (116 партиций) для оптимизации запросов.

Задача 2 (Aggregation): Подсчёт количества посещений по дням с сортировкой по убыванию.

Задача 3 (JOIN + Conditional Aggregation): Аналитика посещений по полу (male/female) в разрезе регионов. JOIN таблиц логов, пользователей и IP-регионов, условная агрегация через конструкцию IF.

Задача 4 (Hive Streaming): Трансформация данных через внешний скрипт — массовая замена доменов (.ru → .com) с использованием sed/awk в потоковом режиме. Вывод всех 6 полей после трансформации.

Задача 5 (Sampling & Analysis): Исследование точности семплирования (TABLESAMPLE) на больших данных. Сравнение результатов при разных процентах выборки, построение графика зависимости точности от размера семпла.

Оптимизации: параллельное выполнение (hive.exec.parallel), настройка числа reducer'ов, управление динамическими партициями.

Stack: Apache Hive, HiveQL, RegexSerDe, Hive Streaming (Bash, sed/awk), TABLESAMPLE
